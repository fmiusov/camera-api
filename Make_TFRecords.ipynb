{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFRecords from VOC XML & jpgs\n",
    "\n",
    "THIS IS REDUNDANTE with UnderstandingObjectDetectionExample  \n",
    "Use the other notebook for a full understanding\n",
    "\n",
    "This was taken from ssd-dag repo\n",
    "\n",
    "## Prerequitistes\n",
    "### Input\n",
    "1. you have jpeg images\n",
    "2. you have annotations - XML format, VOC Pascal format standard\n",
    "\n",
    "### Output\n",
    "tfrecords_dir needs to have 3 subdirectories\n",
    "/train\n",
    "/val\n",
    "/test\n",
    "\n",
    "this code leverages the standards and templates as much as possible\n",
    "\n",
    "## [OPTIONAL] Testing / Visualizing\n",
    "This notebook also includes testing your tfrecord files by visualization.  Two methods:\n",
    "- matplotlib\n",
    "- (tensorflow/models)  object_detection.utils  (This is the preferred method)\n",
    "Remember - you don't have a Linux desktop so, you can't use a GTK based solution like OpenCV for the display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "# - if you don't know what this means\n",
    "#   Look at the notebook TrainModel_Step1_Local\n",
    "#      in this notebook, you basically set up the project with includes cloning \n",
    "#      and compiling the tensorflow/models repo\n",
    "#   we are using the utilities found in that repo\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# this path is different in this project\n",
    "models = os.path.abspath(os.path.join(cwd, '..', 'models/research/'))\n",
    "slim = os.path.abspath(os.path.join(cwd, '..', 'models/research/slim'))\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils.visualization_utils import STANDARD_COLORS\n",
    "from object_detection.utils.visualization_utils import draw_bounding_box_on_image\n",
    "\n",
    "# this is the standard feature dict\n",
    "from example_utils import feature_obj_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_utils import voc_to_tfrecord_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "# this won't work with Tensorflow 2.0\n",
    "# if you have TF 2.0 loaded, you can't set eager - it's forced on\n",
    "\n",
    "print ('TensorFlow Version:', tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "\n",
    "This has been simplified from the ssd-dag project.   Kinda going back and forth between projects.  But there is no CODE and DATA -- everything is together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "\n",
    "IMAGE_DIR = os.path.join(PROJECT, \"jpeg_images\")\n",
    "ANNOTATION_DIR = os.path.join(PROJECT, \"annotation\")\n",
    "LABEL_MAP_FILE = os.path.join(PROJECT, 'model', 'mscoco_label_map.pbtxt')\n",
    "TFRECORD_DIR = os.path.join(PROJECT, 'tfrecord')\n",
    "TRAINING_SPLIT_TUPLE =  (60,30,10)\n",
    "INCLUDE_CLASSES = 'all'\n",
    "EXCLUDE_TRUNCATED = False,\n",
    "EXCLUDE_DIFFICULT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating .tfrecord files from XML annotations & jpeg images\n",
    "\n",
    "## Fix Labels\n",
    "\n",
    "if you get an error like:  \n",
    "!!! label map error: 20190625_polySauce_spicyBag_1561494037 smallSauce  skipped\n",
    "\n",
    "This is telling you that the image_id:  20190625_polySauce_spicyBag_1561494037  \n",
    "has a class label:  smallSauce  \n",
    "which is not defined in the label map.  (don't be fooled!  'smallSauce' is the problem, not polySauce in the filename)\n",
    "\n",
    "If there are a few - you could ignore it.   To fix the data locally:\n",
    "1. review the label_map - $ cat code/cfa_prod_label_map.pbtxt;   youll see 7 == cfaSauce, 10 == polySauce, \n",
    "2. you need to change any 'smallSauce' to one of the labels in the label_map; we will choose cfaSauce \n",
    "3. add a sed command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to fix the labels\n",
    "# it's in the other notebook:  UnderstaningObjectDetectionExample\n",
    "\n",
    "# this is the label_map as a dict\n",
    "#    {'smallHotDrink': 2, 'nuggBox': 5, 'sandBag': 6, 'smallFry': 8, \n",
    "#     'largeFry': 9, 'cfaSauce': 7, 'mediumColdDrink': 3, 'sandBox': 4, \n",
    "#   'hand': 1, 'spicyBag': 11, 'polySauce': 10}\n",
    "\n",
    "# you might have to replace some names due to inconsistencies in labeling and the map\n",
    "\n",
    "! sed -i 's/smHotDrink/smallHotDrink/g' data/annotations/*.xml\n",
    "! sed -i 's/medColdDrink/mediumColdDrink/g' data/annotations/*.xml\n",
    "! sed -i 's/smallSauce/cfaSauce/g' data/annotations/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voc_to_tfrecord_file()\n",
    "This program is in code/cfa_utils/example_utils.py    (Reminder - this isn't an example, it is tf.Example - ugh!) \n",
    "\n",
    "This progrm leverages as much of the standard TensorFlow code as possible.  That means:\n",
    "- annotations are based on VOC PASCAL data standards.   There are hundreds of program examples that use this data which is an XML annotation format.\n",
    "- the tf.Example(Feature) format is based on the format used in the MobileNet model.    I lifted it out of the /models code and placed it here where you can import it.  It is important that you have a consistent format through tfrecord generation, training, predictoin.   \n",
    "\n",
    "I used a pattern where I imported the SSD Feature dictionary then copied it to a dict - then used that dict in the serialization.   You'll see that in the program.   The point is, the feature (dict) format is defined only once in one place.    (Look at the code.)  Odd side effect:   It seems that you must define every element of the dict.  If you don't, you'll get an error:  \n",
    "--> 214             features = tf.train.Features(feature=feature)\n",
    "    215 \n",
    "    216             tf_example = tf.train.Example(features=features)\n",
    "\n",
    "TypeError: MergeFrom() takes exactly one argument (3 given)\n",
    "\n",
    "This program will tell you if it skips image/annotations due to bad labels.  (explained above).\n",
    "\n",
    "### Result:\n",
    "\n",
    "This is telling you that 3149 had a 'verified' (XML attribute) annotation and 22 were not verified.   That is normal.  WHen you label (using labelImg for example), you can skip a questionable training image by simply not verifying it.\n",
    "\n",
    "This dict also shows label map, e.g. hand == class_id = 1\n",
    "\n",
    "  verified: 3149   not: 22\n",
    "{'hand': 1, 'smallHotDrink': 2, 'mediumColdDrink': 3, 'sandBox': 4, 'nuggBox': 5, 'sandBag': 6, 'cfaSauce': 7, 'smallFry': 8, 'largeFry': 9, 'polySauce': 10, 'spicyBag': 11}\n",
    "\n",
    "This is telling you 1889 images were written to the train.tfrecord file.  (not sharded)  \n",
    "169 objects were class_id = 6 (sandBag)  \n",
    "568 objects were class_id = 4 (nuggBox)  \n",
    "These totals will sum >= 1889 because there may be multiple objects per image.\n",
    "\n",
    " -- images 1889  writing to: /home/ec2-user/SageMaker/ssd-dag/tmp/train/train.tfrecord\n",
    "     image count: 1889   class_count: {6: 169, 9: 286, 5: 563, 11: 178, 2: 441, 4: 568, 8: 291, 1: 927, 3: 572, 10: 412, 7: 157}\n",
    "     \n",
    "### file output\n",
    "NOTE - these files were written (depending on your GLOBAL value) to /tmp.    Write to tmp, then promote to S3 if you want to use these.    Look at the training program (notebook) to see where it pulls tfrecords (hint: it won't be /tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /home/ec2-user/SageMaker/ssd-dag/tmp/train\n",
    "! mkdir -p /home/ec2-user/SageMaker/ssd-dag/tmp/test\n",
    "! mkdir -p /home/ec2-user/SageMaker/ssd-dag/tmp/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  verified: 51   not: 1\n",
      "{'/m/01g317': 1, '/m/0199g': 2, '/m/0k4j': 3, '/m/04_sv': 4, '/m/05czz6l': 5, '/m/01bjv': 6, '/m/07jdr': 7, '/m/07r04': 8, '/m/019jd': 9, '/m/015qff': 10, '/m/01pns0': 11, '/m/02pv19': 13, '/m/015qbp': 14, '/m/0cvnqh': 15, '/m/015p6': 16, '/m/01yrx': 17, '/m/0bt9lr': 18, '/m/03k3r': 19, '/m/07bgp': 20, '/m/01xq0k1': 21, '/m/0bwd_0j': 22, '/m/01dws': 23, '/m/0898b': 24, '/m/03bk1': 25, '/m/01940j': 27, '/m/0hnnb': 28, '/m/080hkjn': 31, '/m/01rkbr': 32, '/m/01s55n': 33, '/m/02wmf': 34, '/m/071p9': 35, '/m/06__v': 36, '/m/018xm': 37, '/m/02zt3': 38, '/m/03g8mr': 39, '/m/03grzl': 40, '/m/06_fw': 41, '/m/019w40': 42, '/m/0dv9c': 43, '/m/04dr76w': 44, '/m/09tvcd': 46, '/m/08gqpm': 47, '/m/0dt3t': 48, '/m/04ctx': 49, '/m/0cmx8': 50, '/m/04kkgm': 51, '/m/09qck': 52, '/m/014j1m': 53, '/m/0l515': 54, '/m/0cyhj_': 55, '/m/0hkxq': 56, '/m/0fj52s': 57, '/m/01b9xk': 58, '/m/0663v': 59, '/m/0jy4k': 60, '/m/0fszt': 61, '/m/01mzpv': 62, '/m/02crq1': 63, '/m/03fp41': 64, '/m/03ssj5': 65, '/m/04bcr3': 67, '/m/09g1w': 70, '/m/07c52': 72, '/m/01c648': 73, '/m/020lf': 74, '/m/0qjjc': 75, '/m/01m2v': 76, '/m/050k8': 77, '/m/0fx9l': 78, '/m/029bxz': 79, '/m/01k6s3': 80, '/m/0130jx': 81, '/m/040b_t': 82, '/m/0bt_c3': 84, '/m/01x3z': 85, '/m/02s195': 86, '/m/01lsmm': 87, '/m/0kmg4': 88, '/m/03wvsk': 89, '/m/012xff': 90}\n",
      " -- images 30  writing to: /media/home/jay/projects/camera-api/tfrecord/train/train.tfrecord\n",
      "!!! label map error: 1583021204 potted plant  skipped\n",
      "!!! label map error: 1583021204 person  skipped\n",
      "!!! label map error: 1583021195 person  skipped\n",
      "!!! label map error: 1583021195 potted plant  skipped\n",
      "!!! label map error: 1583021219 person  skipped\n",
      "!!! label map error: 1583021219 backpack  skipped\n",
      "!!! label map error: 1583021219 potted plant  skipped\n",
      "!!! label map error: 1583021220 person  skipped\n",
      "!!! label map error: 1583021220 backpack  skipped\n",
      "!!! label map error: 1583021220 potted plant  skipped\n",
      "!!! label map error: 1583021242 person  skipped\n",
      "!!! label map error: 1583021242 backpack  skipped\n",
      "!!! label map error: 1583021242 book  skipped\n",
      "!!! label map error: 1583021242 potted plant  skipped\n",
      "!!! label map error: 1583021245 person  skipped\n",
      "!!! label map error: 1583021245 book  skipped\n",
      "!!! label map error: 1583021245 backpack  skipped\n",
      "!!! label map error: 1583021245 potted plant  skipped\n",
      "!!! label map error: 1583021194 person  skipped\n",
      "!!! label map error: 1583021241 person  skipped\n",
      "!!! label map error: 1583021241 backpack  skipped\n",
      "!!! label map error: 1583021241 book  skipped\n",
      "!!! label map error: 1583021232 potted plant  skipped\n",
      "!!! label map error: 1583021232 person  skipped\n",
      "!!! label map error: 1583021218 person  skipped\n",
      "!!! label map error: 1583021218 backpack  skipped\n",
      "!!! label map error: 1583021218 potted plant  skipped\n",
      "!!! label map error: 1583021203 potted plant  skipped\n",
      "!!! label map error: 1583021203 person  skipped\n",
      "!!! label map error: 1583021206 person  skipped\n",
      "!!! label map error: 1583021206 potted plant  skipped\n",
      "!!! label map error: 1583021256 potted plant  skipped\n",
      "!!! label map error: 1583021198 person  skipped\n",
      "!!! label map error: 1583021198 potted plant  skipped\n",
      "!!! label map error: 1583021217 person  skipped\n",
      "!!! label map error: 1583021217 backpack  skipped\n",
      "!!! label map error: 1583021217 potted plant  skipped\n",
      "!!! label map error: 1583021249 person  skipped\n",
      "!!! label map error: 1583021249 backpack  skipped\n",
      "!!! label map error: 1583021249 potted plant  skipped\n",
      "!!! label map error: 1583021248 person  skipped\n",
      "!!! label map error: 1583021248 backpack  skipped\n",
      "!!! label map error: 1583021248 book  skipped\n",
      "!!! label map error: 1583021248 potted plant  skipped\n",
      "!!! label map error: 1583021200 person  skipped\n",
      "!!! label map error: 1583021221 person  skipped\n",
      "!!! label map error: 1583021221 backpack  skipped\n",
      "!!! label map error: 1583021221 potted plant  skipped\n",
      "!!! label map error: 1583021226 person  skipped\n",
      "!!! label map error: 1583021226 backpack  skipped\n",
      "!!! label map error: 1583021226 potted plant  skipped\n",
      "!!! label map error: 1583021216 person  skipped\n",
      "!!! label map error: 1583021216 backpack  skipped\n",
      "!!! label map error: 1583021216 potted plant  skipped\n",
      "!!! label map error: 1583021254 potted plant  skipped\n",
      "!!! label map error: 1583021227 person  skipped\n",
      "!!! label map error: 1583021227 potted plant  skipped\n",
      "!!! label map error: 1583021233 person  skipped\n",
      "!!! label map error: 1583021233 backpack  skipped\n",
      "!!! label map error: 1583021240 person  skipped\n",
      "!!! label map error: 1583021240 backpack  skipped\n",
      "!!! label map error: 1583021240 book  skipped\n",
      "!!! label map error: 1583021240 potted plant  skipped\n",
      "!!! label map error: 1583021225 person  skipped\n",
      "!!! label map error: 1583021225 backpack  skipped\n",
      "!!! label map error: 1583021225 potted plant  skipped\n",
      "!!! label map error: 1583021215 person  skipped\n",
      "!!! label map error: 1583021215 backpack  skipped\n",
      "!!! label map error: 1583021224 person  skipped\n",
      "!!! label map error: 1583021224 backpack  skipped\n",
      "!!! label map error: 1583021224 potted plant  skipped\n",
      "!!! label map error: 1583021201 person  skipped\n",
      "!!! label map error: 1583021251 person  skipped\n",
      "!!! label map error: 1583021251 backpack  skipped\n",
      "!!! label map error: 1583021251 book  skipped\n",
      "!!! label map error: 1583021251 potted plant  skipped\n",
      "     image count: 30   class_count: {}\n",
      " -- images 15  writing to: /media/home/jay/projects/camera-api/tfrecord/val/val.tfrecord\n",
      "!!! label map error: 1583021247 person  skipped\n",
      "!!! label map error: 1583021247 backpack  skipped\n",
      "!!! label map error: 1583021247 book  skipped\n",
      "!!! label map error: 1583021247 potted plant  skipped\n",
      "!!! label map error: 1583021228 person  skipped\n",
      "!!! label map error: 1583021228 backpack  skipped\n",
      "!!! label map error: 1583021228 potted plant  skipped\n",
      "!!! label map error: 1583021222 person  skipped\n",
      "!!! label map error: 1583021222 backpack  skipped\n",
      "!!! label map error: 1583021222 potted plant  skipped\n",
      "!!! label map error: 1583021250 person  skipped\n",
      "!!! label map error: 1583021250 backpack  skipped\n",
      "!!! label map error: 1583021250 potted plant  skipped\n",
      "!!! label map error: 1583021253 person  skipped\n",
      "!!! label map error: 1583021253 book  skipped\n",
      "!!! label map error: 1583021253 backpack  skipped\n",
      "!!! label map error: 1583021205 person  skipped\n",
      "!!! label map error: 1583021205 potted plant  skipped\n",
      "!!! label map error: 1583021199 person  skipped\n",
      "!!! label map error: 1583021199 potted plant  skipped\n",
      "!!! label map error: 1583021231 potted plant  skipped\n",
      "!!! label map error: 1583021231 person  skipped\n",
      "!!! label map error: 1583021231 backpack  skipped\n",
      "!!! label map error: 1583021243 person  skipped\n",
      "!!! label map error: 1583021243 book  skipped\n",
      "!!! label map error: 1583021243 backpack  skipped\n",
      "!!! label map error: 1583021243 potted plant  skipped\n",
      "!!! label map error: 1583021238 potted plant  skipped\n",
      "!!! label map error: 1583021229 person  skipped\n",
      "!!! label map error: 1583021229 backpack  skipped\n",
      "!!! label map error: 1583021202 person  skipped\n",
      "!!! label map error: 1583021202 potted plant  skipped\n",
      "!!! label map error: 1583021197 person  skipped\n",
      "!!! label map error: 1583021197 potted plant  skipped\n",
      "!!! label map error: 1583021252 person  skipped\n",
      "!!! label map error: 1583021252 backpack  skipped\n",
      "!!! label map error: 1583021252 book  skipped\n",
      "!!! label map error: 1583021252 potted plant  skipped\n",
      "!!! label map error: 1583021236 potted plant  skipped\n",
      "     image count: 15   class_count: {}\n",
      " -- images 6  writing to: /media/home/jay/projects/camera-api/tfrecord/test/test.tfrecord\n",
      "!!! label map error: 1583021246 person  skipped\n",
      "!!! label map error: 1583021246 book  skipped\n",
      "!!! label map error: 1583021246 potted plant  skipped\n",
      "!!! label map error: 1583021230 person  skipped\n",
      "!!! label map error: 1583021230 backpack  skipped\n",
      "!!! label map error: 1583021230 potted plant  skipped\n",
      "!!! label map error: 1583021223 person  skipped\n",
      "!!! label map error: 1583021223 backpack  skipped\n",
      "!!! label map error: 1583021223 potted plant  skipped\n",
      "!!! label map error: 1583021237 potted plant  skipped\n",
      "!!! label map error: 1583021196 person  skipped\n",
      "!!! label map error: 1583021196 potted plant  skipped\n",
      "!!! label map error: 1583021244 person  skipped\n",
      "!!! label map error: 1583021244 backpack  skipped\n",
      "!!! label map error: 1583021244 book  skipped\n",
      "!!! label map error: 1583021244 potted plant  skipped\n",
      "     image count: 6   class_count: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_to_tfrecord_file(IMAGE_DIR,\n",
    "                    ANNOTATION_DIR,\n",
    "                    LABEL_MAP_FILE,\n",
    "                    TFRECORD_DIR,\n",
    "                    TRAINING_SPLIT_TUPLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Test your TFRecords \n",
    "Select your source of tfrecords\n",
    "data/tfrecords is the source used in training.  \n",
    "tmp is the source you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/data/tfrecords'\n",
    "TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/tmp'\n",
    "print (TFRECORD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will read a list of files\n",
    "# let's combine train, val & test\n",
    "\n",
    "\n",
    "tfrecord_file_list_input = [os.path.join(TFRECORD_DIR, 'train/train.tfrecord'),\n",
    "                            os.path.join(TFRECORD_DIR, 'val/val.tfrecord'),\n",
    "                            os.path.join(TFRECORD_DIR, 'test/test.tfrecord')]\n",
    "print (\"reading:\", tfrecord_file_list_input)\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list_input)\n",
    "raw_dataset.cache()  # cache to memory\n",
    "raw_dataset.shuffle(buffer_size=5000)\n",
    "print (type(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the TFRecords\n",
    "- The file is read into a dataset (TFRecordDatasetV1 to be exact)\n",
    "- Iterating through the records:\n",
    "  - each record is an EagerTensor (you must have Eager Execution enabled)\n",
    "  - This tensor has a serialized tf.Example\n",
    "      - byte string\n",
    "      - get the value (byte string) with .numpy()\n",
    "  - parse the serialized byte string into an Example\n",
    "      - tf.Example is made of Features\n",
    "          - feature[key] == each part of the observation or data point\n",
    "          \n",
    "So, make sure this is correct.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this iteration will show you:\n",
    "# - each record\n",
    "# - each tf.Example\n",
    "# BUT - it is not a parsed tf.Example,  it looks readable, but it's not yet consumable\n",
    "#   look at the next code block for that\n",
    "\n",
    "# VERIFY your mapping using this loop\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    print(\"raw record type:\", type(raw_record))  # serialized Example\n",
    "    print(\"Tensor.dtype:\", raw_record.dtype)\n",
    "    print(\"       value:\", raw_record.numpy()[:50], '\\n')\n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())  # Parse will de-serialize it\n",
    "    # review this to verify the features were mapped correctly\n",
    "    print(type(example), '\\n', example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing each tf.Example record\n",
    "\n",
    "This parses the serialized tf.Example into the feature (dict).   This is where we use that common feature definition to make sure the format is good.   feature_obj_detect is imported from:\n",
    "code.cfa_utils.example_utils.py  \n",
    "\n",
    "This isn't something I defined - I lifted it out of the code in tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"feature_obj_detect:\", type(feature_obj_detect), '\\n', feature_obj_detect)\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input using the standard dictionary\n",
    "    feature = tf.io.parse_single_example(example_proto, feature_obj_detect)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "print (parsed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ref:  UnderstandingTF_IO notebook\n",
    "this will show you some of the tf.io utilities that we are using here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matplotlib\n",
    "\n",
    "mcolors_unique = ['b','g','r','c','m','y','k']  # built in colors\n",
    "mcolors = mcolors_unique + mcolors_unique       # 2x tso we have enough\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(3)):\n",
    "    print (\"record type:\", type(i))\n",
    "    print (\"image/encoded type:\", type(i['image/encoded']))\n",
    "    image_tensor = i['image/encoded'].numpy()  # bytes\n",
    "    print (\"image/encoded EagerTensor.numpy():\", type(image_tensor))\n",
    "    print(\"is jpeg:\", tf.io.is_jpeg(image_tensor))\n",
    "    \n",
    "    jpeg_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    jpeg_numpy = jpeg_decoded_tensor.numpy()\n",
    "    print (\"tf.image.decode_jpeg(image_tensor):\", jpeg_numpy.shape)\n",
    "    \n",
    "    # get height/width\n",
    "    height = i['image/height'].numpy()\n",
    "    width =  i['image/width'].numpy()\n",
    "    \n",
    "    # get object classes\n",
    "    obj_class_names = i['image/object/class/text'].values.numpy()\n",
    "    obj_class_ids = i['image/object/class/label'].values.numpy()\n",
    "    obj_count = len(obj_class_ids)\n",
    "    \n",
    "    print (obj_class_names)\n",
    "    # get the bounding box coordinates\n",
    "    xmins = i['image/object/bbox/xmin'].values.numpy()\n",
    "    xmaxs = i['image/object/bbox/xmax'].values.numpy()\n",
    "    ymins = i['image/object/bbox/ymin'].values.numpy()\n",
    "    ymaxs = i['image/object/bbox/ymax'].values.numpy()\n",
    "    print ('xmins:', type(xmins), xmins)\n",
    "    xmins_pixel = xmins * width\n",
    "    xmaxs_pixel = xmaxs * width\n",
    "    ymins_pixel = ymins * height\n",
    "    ymaxs_pixel = ymaxs * height\n",
    "    print (xmins_pixel)\n",
    "    \n",
    "    # display with \n",
    "    #plt.figure(figsize=(16,16))\n",
    "    #plt.subplot(2,2,n+1)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(jpeg_numpy)\n",
    "    \n",
    "    for idx in range(obj_count):\n",
    "        rect_height = ymaxs_pixel[idx] - ymins_pixel[idx]\n",
    "        rect_width =  xmaxs_pixel[idx] - xmins_pixel[idx]\n",
    "        rect = patches.Rectangle((xmins_pixel[idx],ymins_pixel[idx]),rect_width,rect_height,\n",
    "                                 linewidth=1,edgecolor=mcolors[obj_class_ids[idx]],facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    #plt.imshow(jpeg_numpy)\n",
    "    #currentAxis = plt.gca()\n",
    "    #currentAxis.add_patch(Rectangle((100 - 50, 100 - 50), 0.2, 0.2,\n",
    "    #                  alpha=1, facecolor='none'))\n",
    "    #plt.grid(False)\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/models/object_detection/visualization_util.py\n",
    "\n",
    "THIS is the way to display - much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow/models/object_detection\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(10)):\n",
    "    print (\"record type:\", type(i))\n",
    "    print (\"image/encoded type:\", type(i['image/encoded']))\n",
    "    image_tensor = i['image/encoded'].numpy()  # bytes\n",
    "    print (\"image/encoded EagerTensor.numpy():\", type(image_tensor))\n",
    "    print(\"is jpeg:\", tf.io.is_jpeg(image_tensor))\n",
    "    \n",
    "    jpeg_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    jpeg_numpy = jpeg_decoded_tensor.numpy()\n",
    "    print (\"tf.image.decode_jpeg(image_tensor):\", jpeg_numpy.shape)\n",
    "    \n",
    "    # get height/width\n",
    "    height = i['image/height'].numpy()\n",
    "    width =  i['image/width'].numpy()\n",
    "    \n",
    "    # get object classes\n",
    "    obj_class_names = i['image/object/class/text'].values.numpy()\n",
    "    obj_class_ids = i['image/object/class/label'].values.numpy()\n",
    "    obj_count = len(obj_class_ids)\n",
    "    \n",
    "    print (type(obj_class_names), obj_class_names)\n",
    "    # get the bounding box coordinates\n",
    "    xmins = i['image/object/bbox/xmin'].values.numpy()\n",
    "    xmaxs = i['image/object/bbox/xmax'].values.numpy()\n",
    "    ymins = i['image/object/bbox/ymin'].values.numpy()\n",
    "    ymaxs = i['image/object/bbox/ymax'].values.numpy()\n",
    "    print ('xmins:', type(xmins), xmins)\n",
    "    xmins_pixel = xmins * width\n",
    "    xmaxs_pixel = xmaxs * width\n",
    "    ymins_pixel = ymins * height\n",
    "    ymaxs_pixel = ymaxs * height\n",
    "   \n",
    "    pil_image = Image.fromarray(jpeg_numpy)    \n",
    "    for idx in range(obj_count):\n",
    "        draw_bounding_box_on_image(pil_image,ymins[idx],xmins[idx], ymaxs[idx], xmaxs[idx],\n",
    "                                  color=STANDARD_COLORS[obj_class_ids[idx]], \n",
    "                                  thickness=4, display_str_list=[obj_class_names[idx]],\n",
    "                                  use_normalized_coordinates=True)\n",
    "        \n",
    "    display.display(pil_image)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camera-api)",
   "language": "python",
   "name": "camera-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
